{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import chain \n",
    "from itertools import groupby\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_path = 'Contribution/'\n",
    "layers = ['conv','rnn_0','rnn_1','rnn_2','rnn_3','rnn_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_data/final-file-info.json', 'r') as j:\n",
    "\tfile_meta = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        file   accent  duration\n",
      "0      common_voice_en_22042  england     2.880\n",
      "1    common_voice_en_1177263  england     4.464\n",
      "2   common_voice_en_17287554  england     1.584\n",
      "3     common_voice_en_118123  england     4.344\n",
      "4     common_voice_en_214810  england     2.784\n",
      "5     common_voice_en_205613  england     2.784\n",
      "6   common_voice_en_18130438  england     2.184\n",
      "7     common_voice_en_216610  england     7.536\n",
      "8     common_voice_en_485543  england     3.024\n",
      "9     common_voice_en_606891  england     5.184\n",
      "10     common_voice_en_54962  england     2.832\n",
      "11  common_voice_en_17456660  england     5.760\n",
      "12    common_voice_en_553961  england     3.456\n",
      "13    common_voice_en_247127  england     3.096\n",
      "14    common_voice_en_311828  england     3.144\n",
      "15    common_voice_en_125156  england     5.232\n",
      "16    common_voice_en_517178  england     4.344\n",
      "17  common_voice_en_17254685  england     3.696\n",
      "18      common_voice_en_3403  england     6.816\n",
      "19     common_voice_en_83350  england     5.616\n"
     ]
    }
   ],
   "source": [
    "df_trans = pd.read_csv('my_data/probes/test_1750.csv', header = None, names = ['file', 'accent', 'duration'])\n",
    "#df_trans['file'] = df_trans['file'].map(lambda x: x.split('.')[0])\n",
    "print(df_trans.head(20))\n",
    "files_list = df_trans.file.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750\n"
     ]
    }
   ],
   "source": [
    "print(len(files_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map time axis of representation to the input frames as per the convolutional layers used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_frame(current_frame):\n",
    "    return (current_frame - 1)*2 + 11 - 2*5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_phones = ['iy','ih','ix','ey','eh','er','ae','aa','ao','ay','aw','ah','ax','axr','ow','oy','uh','uw','ux']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate frame level allignments and other related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_allignment(file, input_size):\n",
    "    alligned = []\n",
    "    \n",
    "    spec_stride = 0.01\n",
    "    window_size = 0.02\n",
    "    times = file_meta[file]['end_times']\n",
    "    last_idx = 0\n",
    "    \n",
    "#     print(times[55:58])\n",
    "    for i in range(input_size):\n",
    "        frame_idx = i\n",
    "        window_start = frame_idx*spec_stride\n",
    "        window_mid = window_start + (window_size/2)\n",
    "        alligned_phone = 'na'\n",
    "        \n",
    "        \n",
    "        for j in range(len(times)):\n",
    "            \n",
    "            if (window_mid < times[j]):\n",
    "                #alligned_phone = file_meta[file]['phones'][j]\n",
    "                #print(j)\n",
    "                alligned_phone = j\n",
    "                if(j == 0 and file_meta[file]['phones'][j] =='pause'):\n",
    "                    alligned_phone = -1 # marker for start pause\n",
    "                if(j == len(times)-1 and file_meta[file]['phones'][j] == 'pause'):\n",
    "                    alligned_phone = -2 # marker for end pause\n",
    "                break\n",
    "                \n",
    "        #assert alligned_phone != 'na', \"Failed to fetch allignment\"\n",
    "        if(alligned_phone != 'na'):\n",
    "            alligned.append(alligned_phone)\n",
    "            last_idx = i\n",
    "#     pause_start = 0\n",
    "#     pause_end = len(alligned)\n",
    "#     for i in range(len(alligned)):\n",
    "#         if(alligned[i] != 'pause'):\n",
    "#             break\n",
    "#         pause_start = i\n",
    "    \n",
    "#     for i in range(len(alligned)-1,-1,-1):\n",
    "#         if(alligned[i] != 'pause'):\n",
    "#             break\n",
    "#         pause_end = i\n",
    "        \n",
    "    #print(last_idx)\n",
    "    #print(pause_start, pause_end)\n",
    "#     print(alligned)\n",
    "    allign_grouped = [x[0] for x in groupby(alligned)]\n",
    "    allign_labels = [list(x[1]) for x in groupby(alligned)]\n",
    "    #print(allign_labels)\n",
    "    allign_indices = [0]\n",
    "    for j in allign_labels:\n",
    "        allign_indices.append(allign_indices[-1] + len(j))\n",
    "    #print(allign_indices)\n",
    "    \n",
    "    return allign_grouped, allign_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, -2]\n",
      "[0, 86, 94, 100, 111, 112, 119, 125, 130, 134, 138, 143, 148, 157, 170, 180, 189, 192, 200, 209, 219, 228, 287]\n"
     ]
    }
   ],
   "source": [
    "labels, indices = get_frame_allignment(files_list[0],1000)\n",
    "print(labels)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate allignment of representaions with phones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_labels(file,idx):\n",
    "    spec_stride = 0.01\n",
    "    window_size = 0.02\n",
    "    times = file_meta[file]['end_times']\n",
    "    frame_idx = get_input_frame(idx)\n",
    "    window_start = frame_idx*spec_stride\n",
    "    window_mid = window_start + (window_size/2)\n",
    "    alligned_phone = 'na'\n",
    "    for j in range(len(times)):\n",
    "        if (window_mid < times[j]):\n",
    "            #alligned_phone = file_meta[file]['phones'][j]\n",
    "            alligned_phone = j\n",
    "            if(j == 0 and file_meta[file]['phones'][j] =='pause'):\n",
    "                alligned_phone = -1 # marker for start pause\n",
    "            if(j == len(times)-1 and file_meta[file]['phones'][j] == 'pause'):\n",
    "                alligned_phone = -2 # marker for end pause\n",
    "            break\n",
    "    assert alligned_phone!= 'na', 'found na allignments'\n",
    "    if_vowel = False\n",
    "    if(alligned_phone >= 0):\n",
    "        if_vowel = file_meta[file]['phones'][alligned_phone] in vowel_phones\n",
    "    return alligned_phone, if_vowel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone focus calculations and Neighbour Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixing_accents = {'us':([],[]),'indian':([],[]),'scotland':([],[]),'england':([],[]),'australia':([],[]),'canada':([],[]),'african':([],[])}\n",
    "vowel_accents = {'us':([],[]),'indian':([],[]),'scotland':([],[]),'england':([],[]),'australia':([],[]),'canada':([],[]),'african':([],[])}\n",
    "neighbours = {'us':([],[],[],[],[],[],[]),'indian':([],[],[],[],[],[],[]),'scotland':([],[],[],[],[],[],[]),'england':([],[],[],[],[],[],[]),'australia':([],[],[],[],[],[],[]),'canada':([],[],[],[],[],[],[]),'african':([],[],[],[],[],[],[])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = {'us':([],[],[],[],[],[],[]),'indian':([],[],[],[],[],[],[]),'scotland':([],[],[],[],[],[],[]),'england':([],[],[],[],[],[],[]),'australia':([],[],[],[],[],[],[]),'canada':([],[],[],[],[],[],[]),'african':([],[],[],[],[],[],[])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contribution/rnn_4\n"
     ]
    }
   ],
   "source": [
    "target_layer = 'rnn_4'\n",
    "target_path = os.path.join(contribution_path,target_layer)\n",
    "print(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_list:\n",
    "    #file = 'common_voice_en_55029'\n",
    "    #print(file)\n",
    "\n",
    "#     print(file_meta[file]['end_times'])\n",
    "#     print(file_meta[file]['phones'])\n",
    "    labels, indices = get_frame_allignment(file,1500)\n",
    "#     print(labels)\n",
    "#     print(len(indices))\n",
    "    desired_files = glob(target_path +'/{}*.npy'.format(file))\n",
    "    #print(desired_files)\n",
    "    for f in desired_files:\n",
    "        try:\n",
    "        #print(f)\n",
    "#             print(f)\n",
    "            index = f.split('_')[-2]\n",
    "            if('rnn' in target_layer):\n",
    "                index = f.split('_')[-3]\n",
    "                \n",
    "            #print(index)\n",
    "\n",
    "            #print(index)\n",
    "            out, if_vowel = get_rep_labels(file,int(index))\n",
    "            if(out<0):\n",
    "                continue\n",
    "            out_id = labels.index(out)\n",
    "    #         print(out)\n",
    "    #         print(labels)\n",
    "    #         print(indices)\n",
    "            arr = np.load(f)\n",
    "            if(np.sum(arr) == 0):\n",
    "                #print('encountered all 0')\n",
    "                continue\n",
    "            #plt.plot(arr)\n",
    "            #print(arr[:50])\n",
    "            #print(np.sum(arr))\n",
    "    #         print(len(arr))\n",
    "    #         print(arr[indices[out]-20:indices[out + 1]+20])\n",
    "            sliced = arr[indices[out_id]:indices[out_id + 1]]\n",
    "            contr = np.sum(sliced)*100\n",
    "            if(np.isnan(contr)):\n",
    "                continue\n",
    "            \n",
    "            #print(contr)\n",
    "\n",
    "            all_contr = []\n",
    "            for i in range(len(labels)):\n",
    "\n",
    "    #             if(labels[i] <0):\n",
    "    #                 continue\n",
    "    # #             print(i)\n",
    "                all_contr.append(np.sum(arr[indices[i]:indices[i + 1]]))\n",
    "            max_idx = np.argmax(np.asarray(all_contr))\n",
    "\n",
    "            cond = (max_idx == out_id)\n",
    "#             print(max_idx,out_id,all_contr[max_idx]*100)\n",
    "            #print(contr, cond, max_idx, out_id)\n",
    "            accent = file_meta[file]['accent']\n",
    "            mixing_accents[accent][0].append(contr)\n",
    "            mixing_accents[accent][1].append(cond)\n",
    "            if(if_vowel):\n",
    "                vowel_accents[accent][0].append(contr)\n",
    "                vowel_accents[accent][1].append(cond)\n",
    "        except:\n",
    "            #print('failed for file:',file,index)\n",
    "            continue\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contr:\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "        self.len = len(arr)\n",
    "        #print(arr)\n",
    "        #print('done')\n",
    "    def fetch(self,idx):\n",
    "        #print(self.arr)\n",
    "        if(idx < 0 or idx >= self.len):\n",
    "            #print('enc')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('nor')\n",
    "            return self.arr[idx]\n",
    "    def fetch_range(self,r ):\n",
    "        sum = 0.0\n",
    "        (start, stop) = r\n",
    "        for i in range(start,stop +1):\n",
    "            sum += self.fetch(i)\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_list:\n",
    "    #file = 'common_voice_en_55029'\n",
    "    #print(file)\n",
    "\n",
    "#     print(file_meta[file]['end_times'])\n",
    "#     print(file_meta[file]['phones'])\n",
    "    labels, indices = get_frame_allignment(file,1500)\n",
    "#     print(labels)\n",
    "#     print(len(indices))\n",
    "    desired_files = glob(target_path +'/{}*.npy'.format(file))\n",
    "    #print(desired_files)\n",
    "    for f in desired_files:\n",
    "        try:\n",
    "            #print(f)\n",
    "    #             print(f)\n",
    "            index = f.split('_')[-2]\n",
    "            if('rnn' in target_layer):\n",
    "                index = f.split('_')[-3]\n",
    "\n",
    "            #print(index)\n",
    "\n",
    "            #print(index)\n",
    "            out, if_vowel = get_rep_labels(file,int(index))\n",
    "            if(out<0):\n",
    "                continue\n",
    "            out_id = labels.index(out)\n",
    "    #         print(out)\n",
    "    #         print(labels)\n",
    "    #         print(indices)\n",
    "            arr = np.load(f)\n",
    "            if(np.sum(arr) == 0):\n",
    "                #print('encountered all 0')\n",
    "                continue\n",
    "            #plt.plot(arr)\n",
    "            #print(arr[:50])\n",
    "            #print(np.sum(arr))\n",
    "    #         print(len(arr))\n",
    "    #         print(arr[indices[out]-20:indices[out + 1]+20])\n",
    "            sliced = arr[indices[out_id]:indices[out_id + 1]]\n",
    "            contr = np.sum(sliced)*100\n",
    "            if(np.isnan(contr)):\n",
    "                continue\n",
    "\n",
    "            #print(contr)\n",
    "\n",
    "            all_contr = []\n",
    "            for i in range(len(labels)):\n",
    "\n",
    "    #             if(labels[i] <0):\n",
    "    #                 continue\n",
    "    # #             print(i)\n",
    "                all_contr.append(np.sum(arr[indices[i]:indices[i + 1]]))\n",
    "\n",
    "            max_idx = np.argmax(np.asarray(all_contr))\n",
    "            neigh_contr = Contr(all_contr)\n",
    "\n",
    "            #print(neigh_contr.fetch(0))\n",
    "            cond = (max_idx == out_id)\n",
    "    #             print(max_idx,out_id,all_contr[max_idx]*100)\n",
    "            #print(contr, cond, max_idx, out_id)\n",
    "            accent = file_meta[file]['accent']\n",
    "            #accent = 'timit'\n",
    "    #         mixing_timit[accent][0].append(contr)\n",
    "    #         mixing_timit[accent][1].append(cond)\n",
    "            neighbours[accent][0].append(neigh_contr.fetch(out_id -1) + neigh_contr.fetch(out_id +1))\n",
    "            neighbours[accent][1].append(neigh_contr.fetch(out_id -2) + neigh_contr.fetch(out_id +2))\n",
    "            neighbours[accent][2].append(neigh_contr.fetch(out_id -3) + neigh_contr.fetch(out_id +3))\n",
    "            neighbours[accent][3].append(neigh_contr.fetch_range((out_id - 5, out_id -4)) + neigh_contr.fetch_range((out_id +4, out_id+5)))\n",
    "            neighbours[accent][4].append(neigh_contr.fetch_range((out_id - 8, out_id -6)) + neigh_contr.fetch_range((out_id +6, out_id+8)))\n",
    "            neighbours[accent][5].append(neigh_contr.fetch_range((out_id - 11, out_id -9)) + neigh_contr.fetch_range((out_id +9, out_id+11)))\n",
    "            neighbours[accent][6].append(neigh_contr.fetch_range((out_id - 100, out_id -12)) + neigh_contr.fetch_range((out_id +12, out_id+100)))\n",
    "\n",
    "\n",
    "#             if(if_vowel):\n",
    "#                 vowels_timit[accent][0].append(contr)\n",
    "#                 vowels_timit[accent][1].append(cond)\n",
    "        except:\n",
    "            #print('failed for file:',file,index)\n",
    "            continue\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phone focus and binary phone focus (calculated one layer at a time), shown here for SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us\n",
      "62.54442978064803 25.070968167836934 73.87542644110717\n",
      "indian\n",
      "63.9602528876121 25.894985414401255 74.68457723934468\n",
      "scotland\n",
      "62.3642934689578 25.634266641694573 73.61333169351863\n",
      "england\n",
      "62.21513071446245 25.27033001413601 73.41286889850953\n",
      "australia\n",
      "62.430332227020145 25.491467533674932 73.52960547981294\n",
      "canada\n",
      "61.2306828972328 25.180897913225586 72.55708840692144\n",
      "african\n",
      "64.16524100009383 25.735879414367638 74.89567318251702\n"
     ]
    }
   ],
   "source": [
    "for a in mixing_accents.keys():\n",
    "    try:\n",
    "        print(a)\n",
    "#         \n",
    "        contr_arr = np.asarray(mixing_accents[a][0])\n",
    "#         print(mixing_accents[a][1])\n",
    "#         print(mixing_accents[a][0])\n",
    "#         break\n",
    "        print(contr_arr.mean(),contr_arr.std(),100.0*sum(mixing_accents[a][1])/(len(mixing_accents[a][1])))\n",
    "    except:\n",
    "        continue\n",
    "# for a in vowel_accents.keys():\n",
    "#     try:\n",
    "#         print(a)\n",
    "# #         \n",
    "#         contr_arr = np.asarray(vowel_accents[a][0])\n",
    "# #         print(mixing_accents[a][1])\n",
    "# #         print(mixing_accents[a][0])\n",
    "# #         break\n",
    "#         print(contr_arr.mean(),contr_arr.std(),100.0*sum(vowel_accents[a][1])/(len(vowel_accents[a][1])))\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neighbour Analysis values for layer RNN_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us\n",
      "[22.638480365276337, 12.542382574099609, 7.952615837516744, 9.385792697883067, 7.614086639646633, 4.851851712807524, 14.444850730930911]\n",
      "indian\n",
      "[21.280723810195923, 12.305844389243482, 8.195414496937591, 10.123701281435645, 8.182893904021277, 4.914469927645419, 13.960731343402333]\n",
      "scotland\n",
      "[21.29308432340622, 12.132624955503704, 7.9527269285730995, 9.562700520589063, 7.945317385160462, 4.971957288337058, 15.219619409445926]\n",
      "england\n",
      "[21.94535266152243, 12.496599320776916, 8.170664550136404, 9.697629917171719, 7.984599989060589, 5.00539373145459, 14.395554327181582]\n",
      "australia\n",
      "[22.04861491918564, 12.597264878502923, 8.086613376326612, 9.744694454159763, 8.074721421988148, 4.995766007349288, 13.901542000918287]\n",
      "canada\n",
      "[22.8696346282959, 12.696123461599603, 8.067566000259314, 9.395388820129348, 7.578003661423454, 4.830882189746786, 13.786903177871535]\n",
      "african\n",
      "[21.585925028024786, 11.85953984030524, 7.679363588716809, 9.199444571711155, 7.494925847285976, 4.779391716000584, 15.443010087585678]\n"
     ]
    }
   ],
   "source": [
    "for a in mixing_accents.keys():\n",
    "    try:\n",
    "        print(a)\n",
    "#         \n",
    "        #contr_arr = np.asarray(mixing_accents[a][0])\n",
    "#         print(mixing_accents[a][1])\n",
    "#         print(mixing_accents[a][0])\n",
    "#         break\n",
    "        #print(neighbours[a][0])\n",
    "#         print(100*np.asarray(neighbours[a][0]).mean())\n",
    "        arr = []\n",
    "        for i in range(len(neighbours[a])):\n",
    "            arr.append(100*np.asarray(neighbours[a][i]).mean())\n",
    "        print(arr)\n",
    "        #print(contr_arr.mean(),contr_arr.std(),100.0*sum(mixing_timit[a][1])/(len(mixing_timit[a][1])))\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv \n",
    "# us\n",
    "# [32.7407032251358, 4.129967677019115, 0.5069611499109884, 0.07540823335304898, 0.0020524582412921005, 0.0, 0.0]\n",
    "# indian\n",
    "# [30.90875744819641, 4.187893504497704, 0.6535981730305066, 0.238160239311137, 0.046958024210296455, 0.003738099867418384, 0.0002974208116719145]\n",
    "# scotland\n",
    "# [32.13910460472107, 4.590714934856013, 0.7024039413508787, 0.17588622736358267, 0.02393076658670436, 0.0026134167411263247, 0.0]\n",
    "# england\n",
    "# [32.66556247854946, 4.368573959888614, 0.6079838453871551, 0.11953031163447808, 0.01943777885757124, 0.002580319954526009, 0.0004979741529129851]\n",
    "# australia\n",
    "# [32.34032988548279, 4.472068407601206, 0.632637141106141, 0.11507221019796406, 0.009235619216210154, 1.952351730773e-05, 0.0]\n",
    "# canada\n",
    "# [33.539554476737976, 4.574416147792803, 0.5713643466006877, 0.07515107721887157, 0.007279535102926467, 0.001223377025871137, 0.0]\n",
    "# african\n",
    "# [31.233730379140706, 4.009490048996913, 0.5022208455794831, 0.07889555733309296, 0.00871781443660488, 0.0008204423270378301, 6.012520126919877e-05]\n",
    "\n",
    "# rnn_0\n",
    "# us\n",
    "# [27.49955654144287, 5.13346373603394, 2.0309174663958474, 2.6767877794440995, 3.0200369344228926, 2.362625394259192, 7.852535240260654]\n",
    "# indian\n",
    "# [26.19927227497101, 5.090281970997703, 2.077755862808442, 2.7060146116646786, 2.9382837094459093, 2.2580741789313943, 7.560508918494634]\n",
    "# scotland\n",
    "# [26.919716596603394, 5.392323765776727, 2.1341611514668424, 2.674973549457239, 2.9833333826550628, 2.3151264046490434, 8.07114260450446]\n",
    "# england\n",
    "# [27.37573602958441, 5.295955654012483, 2.1334719217250915, 2.7150068105729908, 3.0929771560113015, 2.4152857833745416, 7.717478458531311]\n",
    "# australia\n",
    "# [27.247267961502075, 5.34691052375551, 2.105283901850338, 2.673661797066982, 3.067990261786627, 2.3905664178446235, 7.4642531857985]\n",
    "# canada\n",
    "# [28.169283270835876, 5.448183687807504, 2.077020231947467, 2.636450567206863, 3.0098093966374746, 2.369359405710461, 7.507328188736076]\n",
    "# african\n",
    "# [26.15024838869291, 4.877831960924951, 1.9293778762854086, 2.5777538785687097, 2.9440899393857025, 2.303586629368907, 8.30029107582431]\n",
    "\n",
    "# rnn_1\n",
    "# us\n",
    "# [23.962603509426117, 8.507628458622953, 4.465992478354428, 5.71455936704617, 6.195458850237518, 4.829749567876604, 16.16509212889644]\n",
    "# indian\n",
    "# [23.195047676563263, 8.404100489357473, 4.470128897364782, 5.671139707643684, 6.008702689557492, 4.620739478180099, 15.721930947546001]\n",
    "# scotland\n",
    "# [23.04048240184784, 8.462198564993297, 4.534059670150146, 5.696610933284059, 6.151703288604992, 4.786315666033271, 16.790108252422357]\n",
    "# england\n",
    "# [23.631045862128857, 8.5951688668769, 4.632277741647281, 5.784687362959637, 6.338151672549959, 4.936726220444522, 16.040685559373344]\n",
    "# australia\n",
    "# [23.752808570861816, 8.661556305105101, 4.572080661245361, 5.7502741717005055, 6.307751578932401, 4.898818396068181, 15.481466421210937]\n",
    "# canada\n",
    "# [24.332773685455322, 8.688939460844866, 4.547470026116145, 5.6507561064439, 6.15434375223156, 4.845393838018302, 15.452406634770167]\n",
    "# african\n",
    "# [22.747576780279, 8.000965653438755, 4.25131166990766, 5.47551236200026, 6.0265592081826975, 4.722378272694236, 17.231608823099382]\n",
    "\n",
    "# rnn_2\n",
    "# us\n",
    "# [24.472643435001373, 11.416035009853484, 6.433550829242235, 7.172085362697694, 6.3121468964634735, 4.558863821094617, 14.998742030734071]\n",
    "# indian\n",
    "# [23.51335436105728, 11.387106717103787, 6.649511262522023, 7.51940392086301, 6.3137742982306, 4.358723318068517, 14.450793554558317]\n",
    "# scotland\n",
    "# [23.225121200084686, 11.172301102451147, 6.492206765380377, 7.298659402172287, 6.401033581395596, 4.550037674420537, 15.69172966370581]\n",
    "# england\n",
    "# [23.913722893040283, 11.459426143523336, 6.633741471145262, 7.348491074447801, 6.48838323695089, 4.668961221382211, 14.981185589184959]\n",
    "# australia\n",
    "# [24.096539616584778, 11.600811539104825, 6.5722590834003265, 7.360531804260481, 6.491923510091799, 4.633371833046721, 14.393180537147854]\n",
    "# canada\n",
    "# [24.683956801891327, 11.572113914395796, 6.535514317857894, 7.151745245651895, 6.264582652847311, 4.568181247712387, 14.349837376059055]\n",
    "# african\n",
    "# [23.278117922402455, 10.784819814874142, 6.167853194400013, 6.9110016395469005, 6.130243824584725, 4.448534439984776, 16.056985357788058]\n",
    "\n",
    "# rnn_3\n",
    "# us\n",
    "# [23.23090434074402, 12.174146764277422, 7.504013566115788, 8.723696850925187, 7.227962272073496, 4.761639574511886, 14.509863650139298]\n",
    "# indian\n",
    "# [21.902601420879364, 12.02527095338879, 7.787259836802122, 9.424352349689228, 7.673399363880054, 4.747935228341197, 13.992496343298566]\n",
    "# scotland\n",
    "# [21.891167759895325, 11.795758316695142, 7.507693259656952, 8.898633270182273, 7.511580442325496, 4.847434087985344, 15.216503226337569]\n",
    "# england\n",
    "# [22.57687249452284, 12.14707995605362, 7.730428943421168, 9.010231605380561, 7.541451811973519, 4.898843390369724, 14.478454061859688]\n",
    "# australia\n",
    "# [22.68569767475128, 12.253803979128968, 7.636378218902687, 9.0469665974413, 7.621676064515722, 4.872514438164382, 13.939816792289179]\n",
    "# canada\n",
    "# [23.474161326885223, 12.297419011472826, 7.59384786153115, 8.712447004216948, 7.161918183838477, 4.73416186686994, 13.869042442307574]\n",
    "# african\n",
    "# [22.116470338617592, 11.47845570845182, 7.220993539734151, 8.530101589118326, 7.0719255800387915, 4.653464676917603, 15.48961048674086]\n",
    "\n",
    "# rnn_4\n",
    "# us\n",
    "# [22.638480365276337, 12.542382574099609, 7.952615837516744, 9.385792697883067, 7.614086639646633, 4.851851712807524, 14.444850730930911]\n",
    "# indian\n",
    "# [21.280723810195923, 12.305844389243482, 8.195414496937591, 10.123701281435645, 8.182893904021277, 4.914469927645419, 13.960731343402333]\n",
    "# scotland\n",
    "# [21.29308432340622, 12.132624955503704, 7.9527269285730995, 9.562700520589063, 7.945317385160462, 4.971957288337058, 15.219619409445926]\n",
    "# england\n",
    "# [21.94535266152243, 12.496599320776916, 8.170664550136404, 9.697629917171719, 7.984599989060589, 5.00539373145459, 14.395554327181582]\n",
    "# australia\n",
    "# [22.04861491918564, 12.597264878502923, 8.086613376326612, 9.744694454159763, 8.074721421988148, 4.995766007349288, 13.901542000918287]\n",
    "# canada\n",
    "# [22.8696346282959, 12.696123461599603, 8.067566000259314, 9.395388820129348, 7.578003661423454, 4.830882189746786, 13.786903177871535]\n",
    "# african\n",
    "# [21.585925028024786, 11.85953984030524, 7.679363588716809, 9.199444571711155, 7.494925847285976, 4.779391716000584, 15.443010087585678]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed for file: common_voice_en_117181 1 22\n",
    "failed for file: common_voice_en_16666058 1 25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
